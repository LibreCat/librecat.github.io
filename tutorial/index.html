---
layout: default
title: Tutorial &ndash; Getting Started with LibreCat/Catmandu
---
  <div class="span11 offset1">
    <div class="marketing">
     <h1>Catmandu Software Development</h1>
     <h2>Getting Started</h2>

     <p class="lead">Importing, transforming, storing and indexing data should be easy</p>

     <p>Catmandu provides a suite of Perl modules to ease the import, storage, retrieval, export and transformation of metadata records. Combine Catmandu modules with web application frameworks such as <a href="http://plackperl.org/">PSGI/Plack</a>, document stores such as <a href="http://www.mongodb.org/">MongoDB</a> and full text indexes as <a href="http://lucene.apache.org/solr/">Solr</a> to create a rapid development environment for digital library services such as institutional repositories and search engines.</p>

     <div class="row-fluid">
  <h3><a name="download">Download from</a></h3>
        <div class="span5">
          <a href="http://search.cpan.org/search?query=Catmandu&mode=all"><img src="http://st.pimg.net/tucs/img/cpan_banner.png"></a>
          </div>
            <div class="span5">
              <br>
              <iframe src="http://ghbtns.com/github-btn.html?user=librecat&repo=catmandu&type=watch&count=true&size=large" allowtransparency="true" frameborder="0" scrolling="0" width="170" height="30" align="right"></iframe>
              <iframe src="http://ghbtns.com/github-btn.html?user=librecat&repo=catmandu&type=fork&count=true&size=large" allowtransparency="true" frameborder="0" scrolling="0" width="170" height="30" align="right"></iframe>
            </div>
          </div>
        </div>
        

    <hr class="soften">

    <div class="row">


     <div class="span11 offset1">

  <h3><a name="where">Where do we use it?</a></h3>
  	<p>
  In the <a href="http://librecat.org">LibreCat</a> project it is our goal to provide in open source a set of programming components to build up digital libraries services suited to your local needs. Here is an example of the projects we are working on:
  	</p>
  	<ul class="square">
            <li><strong>LibreCat-Catalog</strong> : a next generation institutional repository (in development).</li>
  	  <li><strong>LibreCat-Citation</strong> : a <a href="http://citationstyles.org/">CSL</a> based citation list (in development)</li>
        <li><strong><a href="http://biblio.ugent.be">LibreCat-Search</a></strong> : an <a href="http://www.elasticsearch.org/">ElasticSearch</a> based front-end for institutional repositories. [<a href="https://github.com/Universiteitsbibliotheek/MeerCat">GitHub</a>]</li>
  	  <li><strong><a href="http://adore.ugent.be">LibreCat-Grim</a></strong> : a <a href="http://lucene.apache.org/solr/">Solr</a>/<a href="http://iipimage.sourceforge.net/">IIPImage</a> based image database.</li>
        <li><strong>LibreCat-Archive</a></strong> : a <a href="http://fedora-commons.org/">Fedora Commons</a> based digital repository (in development). [<a href="https://github.com/LibreCat/Catmandu-FedoraCommons">GitHub</a>]</li>
        <li><strong><a href="https://github.com/LibreCat/Imaging">LibreCat-Imaging</a></strong> : a <a href="http://mediamosa.org/">MediaMosa</a> based digitization workflow engine.</li>
  	</ul>

      <p>
  We have more than 40 Catmandu projects available at <a href="https://github.com/organizations/LibreCat">GitHub LibreCat</a>.
      </p>

  	<h3><a name="why">Why do we use it?</a></h3>
          <div style="text-align: center;">
          <img src="../assets/img/catmandu.png">
        </div>
  	<h4><a name="etl">Extract, Transform and Load</a></h4>
  	<p>
  Create a search engine, one of your first tasks will to import data from various sources, map the fields to a common data model and post it to a full-text search engine. Perl modules such as <a href="http://search.cpan.org/~bricas/WebService-Solr-0.17/lib/WebService/Solr.pm">WebService::Solr</a> or <a href="http://search.cpan.org/~drtech/ElasticSearch-0.52/lib/ElasticSearch.pm">ElasticSearch</a> provide easy access to your favorite document stores, but you keep writing a lot of boilerplate code to create the connections, massaging the incoming data into the correct format, validating and uploading and indexing the data in the database. Next morning you are asked to provide a fast dump of records into an Excel worksheet. After some fixes are applied you are asked to upload it into your database. Again you hit Emacs or Vi and provide an ad-hoc script. In our LibreCat group we saw this workflow over and over. We tried to abstract this problem to a set of Perl tools which can work with library data such as MARC, Dublin Core, EndNote protocols such as OAI-PMH, SRU and repositories such as DSpace and Fedora. In data warehouses these processes are called ETL, <i>Extract</i>, <i>Transform</i>, <i>Load</i>. Many tools currenty exist for ETL processing but none adress typical library data models and services.
  	</p>

  	<h4><a name="copypaste">Copy and Paste</a></h4>
  	<p>
  As programmers, we would like to reuse our code and algorithms as easy as possible. In fast application development you typically want to copy and paste parts of existing code in a new project. In Catmandu we use a functional style of programming to keep our code tight and clean and suitable for copy and pasting. When working with library data models we use native Perl hashes and arrays to pass data around. In this way adhere to the rationale of Alan J. Perlis: "It is better to have 100 functions operate on one data structure than to have 10 functions operate on 10 data structures." Our functions are all based on a few primary data structures on which we define many functions (map, count, each, first, take, ...)
  	</p>

  	<h4><a name="nosql">Schemaless databases</a></h4>
  	<p>
  Working with native Perl hashes and arrays we would like to use an easy mechanism to store and index this data in a database of choice. In the past it was a nuisance to create database schemas and indexes to store and search your data. Certainly in institutional repositories this can be a ongoing job for a programmer because the metadata schemas are not fixed in time. Any new report will require you to add new data fields and new relations for which you need to change your database schema. With the introduction of schemaless databases the storage of complex records is really easy. Create a Perl hash excute the function 'add' and your record is stored into the database. Execute 'get' to load a Perl hash from the database in memory. With our <i>ElasticSearch</i> plugin we even can provide you a CQL style query language for retrieval.</p>

{% highlight perl %}
   my $obj = { name => { last => 'Bond' , full => 'James Bond' } , occupation => 'Secret Agent' };
   $store->bag->add($obj);
    
   $store->bag->search(cql_query => 'name.last = Bond')->each(sub {
  	my $obj = shift;
  	printf "%s\n" , $obj->{name}->{full};
   });
{% endhighlight %}


  	<h3><a name="install">Before you start</a></h3>
  	<p>

  To get Catmandu running on your system you need to download and install the <a href="http://search.cpan.org/search?query=Catmandu&mode=all">Catmandu</a>
  module from CPAN. The Task::Catmandu meta-package bundles some additional modules commonly used with Catmandu:</p>

  <p>On a Debian-based Linux distribution, this will install some prerequisites:
{% highlight bash %}
   $ sudo apt-get install build-essential cpanminus libexpat1-dev libxml2-dev libxml-libxml-simple-perl
{% endhighlight %}

  On RPM-based Linux distributions, the crucial binary package you need to install is called <tt>libxml2-devel</tt>.</p>

  <p>Finally, for all systems, the following will install Catmandu and friends with their dependencies fully automatically.
{% highlight bash %}
   $ cpanm Task::Catmandu
{% endhighlight %}
  	</p>


  	<h3><a name="importer">Importer</a></h3>
  	<p>
  Importers are Catmandu packages to read data into an application. We provide importers for MARC, JSON, YAML, CSV, Excel but also Atom and OAI-PMH endpoints.
  	</p>
  	<p>
  As an example, lets create a Perl script to read a YAML file containing an array of values. We use the <tt>each</tt> function to loop through all the items
  	</p>
{% highlight perl %}
  #!/usr/bin/env perl
  use Catmandu::Importer::YAML;
  my $importer = Catmandu::Importer::YAML->new(file => "./test.yaml");

  my $count = $importer->each(sub {
     my $obj = shift;
     # .. your code ..
  });

  print "Read: $count YAML items\n";
{% endhighlight %}
  	<p>
  Running this script using this <a href="test.yaml">test.yaml</a> file you should see as output:
  	</p>
{% highlight sh %}
  Read: 3 YAML items
{% endhighlight %}
  	<p>
  Here is an example script to read 10 records from an OAI-PMH endpoint into an application:
  	</p>

{% highlight perl %}
  #!/usr/bin/env perl
  use Catmandu::Importer::OAI;

  my $importer = Catmandu::Importer::OAI->new(url => 'http://biblio.ugent.be/oai');

  my $count = $importer->take(10)->each(sub {
     my $obj = shift;
     # .. your code ..
  });

  print "Read sample of $count OAI items\n";
{% endhighlight %}
  	
  	<h3><a name="iterable">Iterable</a></h3>
  	<p>
  The Iterable package provides many list methods to process large streams of records. Most of the methods are lazy if the underlying datastream supports it. While all of the data in Catmandu are native Perl hashes and arrays it can be impratical to load a result set of thousands of records into memory. Most Catmandu packages such as Importer, Exporter, Store provide therefor an Iterable implementation.
  	</p>
  	<p>
  Using a 'Mock' importer we can generate some Perl hashes on-the-fly and show the functionality provided by Iterable:
  	</p>

{% highlight perl %}
  use Catmandu::Importer::Mock;
  my $it = Catmandu::Importer::Mock->new(size => 10);
{% endhighlight %}
  	<p>
  With <tt>each</tt> you can loop over all the items in an iterator:
  	</p>
  	{% highlight perl %}
  $it->each(sub {
     printf "My n is %d\n" , shift->{n};
  });
{% endhighlight %}
  	<p>
  Using <tt>any</tt>, <tt>many</tt>, <tt>all</tt> you can test for the existence of items in an Iterator:
  	</p>

{% highlight perl %}
  my $answer = $it->any(sub { shift->{n} > 4});
  printf "Iterator contains n > 4 = %s\n" , $answer ? 'TRUE' : 'FALSE';

  my $answer = $it->many(sub { shift->{n} > 8});
  printf "Iterator contains n > 8 = %s\n" , $answer ? 'TRUE' : 'FALSE';

  my $answer = $it->all(sub { shift->{n} =~ /^\d+$/});
  printf "Iterator contains only digits = %s\n" , $answer ? 'TRUE' : 'FALSE';
{% endhighlight %}

  	<p>
  <tt>Map</tt> and <tt>reduce</tt> are functions that evaluate a function on all the items in an iterator to procude a new iterator or a summary of the results:
  	</p>

{% highlight perl %}
  # $it contains: [ { n => 1 } , { n => 2 } , ... ];
  my $ret = $it->map(sub {
       my $hash = shift;
       { n => $hash->{n} * 2 }
  });

  # $ret contains : [ { n => 2 } , { n => 4 } , ... ];

  my $result = $it->reduce(0,sub {
       my $prev = shift;
       my $this = shift->{n} * 2;
       $prev + $this;
  });
  printf "SUM [ Iterator * 2] = %d\n" , $result;
{% endhighlight %}

  	<p>
  The Iterable package provides many more functions such as: to_array, count, each, first, slice, take, group, tap, detect, select, reject, any, many, all, map, reduce and invoke.
  	</p>

  	<h3>Exporter</h3>
  	<p>
  Exporters are Catmandu packages to export data from an application. As input they can get native Perl hashes or arrays but also Iterators to stream huge data sets.
  	</p>
  	<p>
  Here is an example using our Mock importer to stream 1 million Perl hashes through an Exporter:
  	</p>

  	{% highlight perl %}
  use Catmandu::Exporter::YAML;
  my $exporter = Catmandu::Exporter::YAML->new();
  $exporter->add_many(Catmandu::Importer::Mock->new(size => 1000000));
{% endhighlight %}
  	<p>
  Catmandu provides exporters for BibTeX, CSV, JSON, RIS, XLS and YAML. If you need a special exporter for your own format you could use the Template exporter which uses <a href="http://template-toolkit.org/">Template Toolkit</a>.
  	</p>
  	<p>
  As an example lets create an exporter for an Perl array of hashes <tt>$data</tt> using a template:
  	</p>
{% highlight perl %}
  use Catmandu::Exporter::Template;
  my $data     = [
   { name => { first => 'James' , last => 'Bond' } , occupation => 'Secret Agent' } ,
   { name => { first => 'Ernst' , last => 'Blofeld' } , occupation => 'Supervillain' } ,
  ];
  my $exporter = Catmandu::Exporter::Template->new(template => '/home/phochste/example.tt');
  $exporter->add_many($data);
{% endhighlight %}
  The template <a href="example.tt">example.tt</a> will be rendered for every hash in the array <tt>$data</tt> (or for every item in an Iterable <tt>$data</tt>).
{% highlight html+django %}
  <character>
   <name>[% name.last %], [% name.first %]</name>
   <occupation>[% occupation %]</occupation>
  </character>
{% endhighlight %}

  	<h3>Fix</h3>
  	<p>
  Fixed can be used for easy data manipulation by non programmers. Using a small Perl DSL language librarians can use Fix routines to manipulate data objects. A plain text file of fixes can be created to specify all the data manipulations that need to be executed to 'massage' the data in the desired format.
  	</p>
  	<p>
  As an example we will import data from a MARC file and change some metadata fields using Fix routines. Here is the code to run the example:
  	</p>
  	{% highlight perl %}
  use Catmandu::Fix;
  use Catmandu::Importer::MARC;
  use Data::Dumper;

  my $fixer = Catmandu::Fix->new(fixes => ['marc.fix']);
  my $it    = Catmandu::Importer::MARC->new(file => 'marc.txt', type => 'ALEPHSEQ');

  $fixer->fix($it)->each(sub {
     my $obj = shift;
     print Dumper($obj);
  });
{% endhighlight %}
  	<p>
  The output of this script should generate something like this:
  	</p>
  	{% highlight perl %}
  $VAR1 = {
            '_id' => '000000043',
            'my' => {
                      'authors' => [
                                     'Patrick Hochstenbachhttp://ok',
                                     'Patrick Hochstenbach2My bMy eMy codeGhent1971',
                                     'Patrick Hochstenbach3',
                                     'Stichting Ons Erfdeel'
                                   ],
                      'language' => 'dut',
                      'subjects' => [
                                      'MyTopic1',
                                      'MyTopic2',
                                      'MyTopic3',
                                      'MyTopic4'
                                    ],
                      'stringy' => 'MyTopic1; MyGenre1; MyTopic2; MyGenre2; MyTopic3; MyTopic4; MyGenre4'
                    }
          };
{% endhighlight %}

  	<p>
  We need two files as input: <a href="marc.txt">marc.txt</a> is a file containing MARC records and <a href="marc.fix">marc.fix</a> contains the fixes that need to be applied to each MARC record. Lets take a look at the contents of this <tt>marc.fix</tt> file:
  	</p>
  	{% highlight perl %}
  marc_map('100','my.authors.$append');
  marc_map('710','my.authors.$append');
  marc_map('600x','my.subjects.$append');
  marc_map('008_/35-37','my.language');
  marc_map('600','my.stringy', -join => "; ");
  marc_map('199','my.condition', -value => 'ok');

  remove_field('record');
{% endhighlight %}
  	<p>
  The fixes in this file are specialized in MARC processing. In line 1 we map the contents of the MARC-100 field into a deeply neested Perl hash with key 'authors'. In line 3 we map the contents of the MARC-600 x-subfield into the 'subjects' field. In Line 4 we read characters 35 to 37 from the MARC-008 control field into the 'language' key.
  	</p>
  	<p>
  A Catmandu Fix provides also many functions to manipulate Perl hashes. The <tt>remove_field</tt>, as shown above in the fix file, will remove a key from a Perl hash. Other fix function are: add_field, capitalize, clone, collapse, copy_field, downcase, expand, join_field, move_fild, remove_field, replace_all, retain_field, set_field, split_field, substring, trim and upcase.

  <h3>Store</h3>
  <p>
  As explained in the introduction, one of the rationales for creating Catmandu is to ease the serialization of records in our database of choice.
  The introduction of schemaless databases made the storage of complex records quite easy. Before we delve into this type of database
  we need to show you what syntax Catmandu is using to store data.
  </p>
  <p>
  As example lets create the most simple storage mechanism possible, an in memory hash. We use this mock 'database' to show some
  of the features that any Catmandu::Store has. First we will create a YAML importer as shown above to import records into
  an in memory hash store:
  </p>

{% highlight perl %}
  use Catmandu::Importer::YAML;
  use Catmandu::Store::Hash;
  use Data::Dumper;

  my $importer = Catmandu::Importer::YAML->new(file => "./test.yaml");
  my $store    = Catmandu::Store::Hash->new();

  # Store an iterable
  $store->bag->add_many($importer);

  # Store an array of hashes
  $store->bag->add_many([ { name => 'John' } , { name => 'Peter' }]);

  # Store one hash
  $store->bag->add( { name => 'Patrick' });

  # Commit all changes
  $store->bag->commit;
{% endhighlight %}

  <p>
  Each Catmandu::Store have one or more compartments (e.g. tables) to store data called 'bag'. We use the function 'add_many' to store 
  each item in the importer Iterable into the Store. We can also store an array of Perl hashes with the same command. Or store a 
  single hash with the 'add' method.
  </p>

  <p>
  Each bag is an Iterator so you can apply any of the 'each','any','all',... methods shown above to read data from a bag.
  </p>

{% highlight perl %}
   $store->bag->take(3)->each(sub {
     my $obj = shift;
     #.. your code
   });
{% endhighlight %}

  <p>
  When you store a perl Hash into a Catmandu::Store then an identifier field '_id' gets added to your perl Hash that can be used to 
  retrieve the item at a later stage. Lets take a look at the identifier and how it can be used.
  </p>

{% highlight perl %}
  # First store a perl hash and return the stored item which includes the stored identifier
  my $item = $store->bag->add( { name => 'Patrick' });

  # This will show you an UUID like '414003DC-9AD0-11E1-A3AD-D6BEE5345D14'...
  print $item->{_id} , "\n";

  # Now you can use this identifier to retrieve the object from the store
  my $item2 = $store->bag->get('414003DC-9AD0-11E1-A3AD-D6BEE5345D14');
{% endhighlight %}

  <p>
  And that is how it works. Catmandu::Store has some more functionality to delete items and query the store (if the backend
  supports it), but this is how you can store very complex Perl structures in memory or on disk with just a few lines of
  code. As a complete example we can show how easy it is to store data in a fulltext search engine like ElasticSearch.
  </p>

  <p>
  In this example we will download ElasticSearch version 0.19.3 from <a href="http://www.elasticsearch.org/download/">this</a> website 
  and install it on our system:
  </p>

{% highlight bash %}
  $ wget https://github.com/downloads/elasticsearch/elasticsearch/elasticsearch-0.19.3.tar.gz
  $ tar zxvf elasticsearch-0.19.3.tar.gz
  $ cd elasticsearch-0.19.3
  $ bin/elasticsearch
{% endhighlight %}

  <p>
  After running the last command 'bin/elasticsearch' we have started the search daemon. Now we can index some data with 
  Catmandu:
  </p>

{% highlight perl %}
  use Catmandu::Importer::YAML;
  use Catmandu::Store::ElasticSearch;

  my $importer = Catmandu::Importer::YAML->new(file => './test.yaml');
  my $store    = Catmandu::Store::ElasticSearch->new(index_name => 'demo');
   
  $store->bag->add_many($importer);

  $store->bag->commit;
{% endhighlight %}

  <p>
  All records in the file 'test.yaml' should be available now index. We can test this by executing a new script to
  read all records stored in the store:
  </p>

{% highlight perl %}
  use Catmandu::Store::ElasticSearch;
  use Data::Dumper;

  my $store = Catmandu::Store::ElasticSearch->new(index_name => 'demo');

  $store->bag->each(sub {
    my $obj = shift;
    print Dumper($obj);
  });
{% endhighlight %}

  <p>
  If everything work correct you should something like this:
  </p>

{% highlight perl %}
  $VAR1 = {
            'first' => 'Charly',
            '_id' => '96CA6692-9AD2-11E1-8800-92A3DA44A36C',
            'last' => 'Parker',
            'job' => 'Artist'
          };
  $VAR1 = {
            'first' => 'Joseph',
            '_id' => '96CA87F8-9AD2-11E1-B760-84F8F47D3A65',
            'last' => 'Ratzinger',
            'job' => 'Pope'
          };
  $VAR1 = {
            'first' => 'Albert',
            '_id' => '96CA83AC-9AD2-11E1-B1CD-CC6B8E6A771E',
            'last' => 'Einstein',
            'job' => 'Physicist'
          };
{% endhighlight %}

  <p>
  The ElasticSearch store even provides an implementation of the Lucene and CQL query language:
  </p>

{% highlight perl %}
  my $hits = $store->bag->searcher(query => 'first:Albert');

  $hits->each(sub {
    my $obj = shift;
    printf "%s %s\n" , $obj->{first} , $obj->{last};
  });
{% endhighlight %}

  <p>
  This last example will print 'Albert Einstein' as result. Clinton Gormley did some great work in providing
  a Perl client for ElasticSearch. Searching complex objects can be done by using a dot syntax e.g. 'record.titles.0.subtitle:"My Funny Valentine"'.
  The beauty of ElasticSearch is that it is completely plainless to setup and requires no schema: indexing
  data is simply done by using JSON over HTTP. All your fields are indexed automatically. 
  </p>

  <h3>Lazy</h3>
  <p>
  Most of the Catmandu processing doesn't require you to write any Perl code. With command line tools you can store data files into databases, index your data, export data in various formats and provide basis data cleanup operations.
  </p>
  <p>
  Say, you have a YAML file 'test.yml' like:
  </p>
{% highlight yaml %}
  ---
  first: Charly
  last: Parker
  job: Artist
  ---
  first: Albert
  last: Einstein
  job: Physicist
  ---
  first: Joseph
  last: Ratzinger
  job: Pope
  ...
{% endhighlight %}
  <p>
  and you are required to transform it into JSON. Using the '<tt>catmandu</tt>' command you can do this with these options:
  </p>

{% highlight perl %}
    $ catmandu data --from-importer YAML --into-exporter JSON < test.yml
{% endhighlight %}

  <p>
  Basically you connect a YAML importer to a JSON exporter.
  </p>

  <p>
  Need some fancy export? Then use the Template exporter which uses a template file like 'test.xml.tt' below to render the output.
  </p>

{% highlight xml %}
   <foo>
    <first>[% first %]</first>
    <last>[% last %]</last>
    <job>[% job %]</job>
   </foo>
{% endhighlight %}

  <p>
  To run the 'catmandu' command you need to provide 'Template' as the exporter to write into and a full path to the template file (without the .tt extension). Note that optional arguments for Importers and Exporters can be provided with the '--from-[NAME]' , '--into-[NAME]' syntax:
  </p>

{% highlight bash %}
   $ catmandy data --from-importer YAML --into-exporter Template --into-template `pwd`/test.xml < test.yml
{% endhighlight %}

  <p>
  Which produces the output:
  </p>

{% highlight xml %}
   <foo>
    <first>Charly</first>
    <last>Parker</last>
    <job>Artist</job>
   </foo>
   <foo>
    <first>Albert</first>
    <last>Einstein</last>
    <job>Physicist</job>
   </foo>
   <foo>
    <first>Joseph</first>
    <last>Ratzinger</last>
    <job>Pope</job>
   </foo>
{% endhighlight %}

  <p>
  Using this command line tools indexing data becomes also very easy. Boot up the ElasticSearch and run the command below to index the test.yml file:
  </p>

{% highlight bash %}
   $ catmandu data -v --into-store ElasticSearch --into-index_name demo --into-bag data --from-importer YAML < test.yml
{% endhighlight %}

  <p>
  To show the results from your hard word we can export all the records from the ElasticSearch store:
  </p>

{% highlight bash %}
   $ catmandu data --from-store ElasticSearch --from-bag data --from-index_name demo
{% endhighlight %}
{% highlight json %}
  {"first":"Albert","_id":"3A07B0F8-0973-11E2-98F8-F84380C42756","last":"Einstein","job":"Physicist"}
  {"first":"Charly","_id":"3A0792D0-0973-11E2-8724-A22A2812F5B2","last":"Parker","job":"Artist"}
  {"first":"Joseph","_id":"3A07B5EE-0973-11E2-97BF-E053E6A92BE5","last":"Ratzinger","job":"Pope"}
{% endhighlight %}

  <p>
  We can even be more lazy by creating a catmandu.yml file containing the connection parameters to the ElasticSearch:
  </p>

{% highlight yaml %}
  ---
  store:
   default:
    package: ElasticSearch
    options:
      index_name: demo
{% endhighlight %}

  <p>
  Using the configuration file above indexation of YAML data can be done like this:
  </p>

{% highlight bash %}
   $ catmandu data -v --into-bag data --from-importer YAML < ~/Desktop/test.yaml
{% endhighlight %}

  <p>
  And exporting all data can be done like this:
  </p>

{% highlight bash %}
   $ catmandu data --from-bag data
{% endhighlight %}

  <p>
  For Catmandu stores that support a query language, exporting data can be very powerfull using the '--query' option. E.g. we can export all records about 'Einstein' from our ElasticSearch store using:
  </p>

{% highlight bash %}
   $ catmandu data --from-bag data --query "Einstein"
{% endhighlight %}
  
  <hr class="soften">

  <p><em>If you are interested in writing web applications, then please proceed to part 2 of this tutorial: <a href="dancer.html">Dancer &amp; Catmandu</a>.</em></p>

  </div>
</div>
